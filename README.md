## Case_Vendas_Eng_Dados

## Apresenta√ß√£o do Case
Este reposit√≥rio apresenta um case de engenharia de dados focado na an√°lise detalhada de vendas dos anos de (2011/2012), desenvolvido para validar conhecimentos t√©cnicos na √°rea e ser submetido √† banca examinadora Data Master. O projeto busca explorar o desempenho comercial a partir de m√∫ltiplas perspectivas, incluindo cliente, produto, fabricante, geografia, segmento, categoria e tempo. Al√©m disso, s√£o medidos KPIs essenciais, como o total de vendas e suas varia√ß√µes di√°rias, mensais e anuais, permitindo uma vis√£o estrat√©gica e aprofundada dos dados. Com base nessa an√°lise, o objetivo √© que a √°rea de neg√≥cios possa identificar oportunidades e definir estrat√©gias de crescimento fundamentadas em insights precisos, refor√ßando a import√¢ncia da tomada de decis√£o orientada por dados. 

## I. Objetivo do Case

## Assunto: An√°lise de Vendas

### Explicado por:
- Cliente
- Produto
- Fabricante
- Geografia
- Segmento
- Categoria
- Tempo

## Objetivos da an√°lise:
Este case visa fornecer uma vis√£o detalhada das vendas, identificando padr√µes, oportunidades e insights com base nos dados analisados.

## Modelo Conceitual
![Modelo Conceitual](./Modelo_Conceitual.jpg)

Esse modelo conceitual, se caracteriza com o esquema floco de neve (snow flake), que √© um modelo de dados multidimensional e uma extens√£o do esquema em estrela, subdividindo as tabelas de dimens√£o em tabelas de subdimens√£o. Os esquemas em floco de neve s√£o comumente usados para business intelligence e relat√≥rios usando an√°lise multidimensional em data warehouses, data marts e bancos de dados relacionais.

### M√©tricas analisadas:
- **Total de vendas** (quantidade e valor)
- **Varia√ß√µes de vendas** 
  - Di√°ria (quantidade e valor)
  - Mensal (quantidade e valor)
  - Anual (quantidade e valor)
- **Ticket m√©dio** (quantidade e valor)
- **Total YTD/MTD** (quantidade e valor)
- **Varia√ß√£o do total YTD/MTD** (quantidade e valor)

## Benef√≠cios esperados:
- Melhor compreens√£o do comportamento de vendas em diferentes dimens√µes.
- Identifica√ß√£o de oportunidades estrat√©gicas com base em tend√™ncias e varia√ß√µes de desempenho.
- Suporte √† tomada de decis√£o por meio de dados estruturados e visualiza√ß√µes anal√≠ticas.

## II. Arquitetura de Solu√ß√£o e Arquitetura T√©cnica 

## Arquitetura Medallion com PySpark, Databricks e Delta Lake

Este case foi desenvolvido utilizando **notebooks em PySpark**, com a implementa√ß√£o da arquitetura **Medallion** nas camadas **Bronze, Silver e Gold**, por meio do **Databricks Community e Delta Lake**. Entre as funcionalidades implementadas, destacam-se:

- Cria√ß√£o de **surrogate keys** (chaves substitutas) para as dimens√µes;
- Otimiza√ß√£o da **tabela fato** na camada Gold.

### Estrutura dos Notebooks

Foram desenvolvidos **9 notebooks**, organizados e numerados para facilitar a compreens√£o da ordem de execu√ß√£o:

1. `000 Criando os Diret√≥rios`  
2. `001 Importando o Arquivo`  
3. `002 Load Camada Bronze`  
4. `003 Transforma√ß√µes Camada Silver`  
5. `004 Load Camada Gold Delta`  
6. `005 Load Gold Delta Incremental`  
7. `006 Consultas Otimizadas`  
8. `007 Cria√ß√£o de tabelas Delta`  
9. `008 Rotinas de Manuten√ß√£o Delta`  

Cada notebook cont√©m **explica√ß√µes e justificativas** sobre sua utiliza√ß√£o e implementa√ß√£o, proporcionando uma vis√£o clara do processo.

## üèóÔ∏è Arquitetura Medallion

A Arquitetura Medallion √© um modelo de processamento de dados em camadas que organiza os dados de maneira estruturada e otimizada para an√°lise. Ela √© composta pelas seguintes etapas:

### üîπ Landing Zone
Os dados s√£o armazenados de forma bruta, sem qualquer tipo de processamento. Esta camada √© essencial para garantir que os dados originais estejam dispon√≠veis para auditoria e rastreamento. Outro benef√≠cio da landing zone √© a preserva√ß√£o dos dados brutos, permitindo que, em caso de erro no processo de ETL, seja poss√≠vel reutiliz√°-los sem a necessidade de uma nova extra√ß√£o dos dados de produ√ß√£o. O notebook `001 Importando o Arquivo` cont√©m as importa√ß√£o para a Landing Zone.

### ü•â Bronze
A camada Bronze √© respons√°vel pela importa√ß√£o dos dados brutos. Nesta camada, foi realizado a importa√ß√£o de um arquivo .csv contendo dados fict√≠cios de vendas referentes ao ano de 2011 e 2012 armazenado neste reposit√≥rio com o nome dados_vendas_2011.csv e dados_vendas_2012.csv. Estes s√£o os √∫nicos arquivos a serem importados para a camada Bronze. Os detalhes e justificativas sobre a camada bronze podem ser encontrados no notebook `002 Load Camada Bronze`.

### ü•à Silver
Na camada Silver, os dados s√£o limpos, padronizados e transformados para facilitar a an√°lise. O DataFrame df_bronze √© ajustado para df_silver, convertendo datas, formatando valores monet√°rios, refinando a estrutura dos campos e excluindo colunas desnecess√°rias. Al√©m disso, o nome e o e-mail s√£o mascarados para garantir a prote√ß√£o dos dados. Os detalhes e justificativas sobre a camada silver podem ser encontrados no notebook `003 Transforma√ß√µes Camada Silver`.

### üèÜ Gold
Na camada Gold, os dados da camada Silver s√£o lidos e transformados para a cria√ß√£o de Delta Parquet, estruturando as dimens√µes produto, categoria, segmento, fabricante, geografia, cliente e a fato vendas. Esse processo √© implementado pelo notebook `004 Load Camada Gold Delta`.
Al√©m disso, dentro da mesma camada, o notebook ¬¥005 Load Gold Delta Incremental` √© respons√°vel pela importa√ß√£o de novos arquivos, incluindo a carga nas camadas Bronze e Silver, a atualiza√ß√£o das dimens√µes e a cria√ß√£o da fato vendas.
Como estou utilizando a vers√£o Community do Databricks, n√£o foi poss√≠vel automatizar esse fluxo via workflow, tornando necess√°rio realizar a carga manualmente, executando os notebooks `001`, `002`, `003` e `004`. Todas as justificativas e detalhes sobre esse processo est√£o documentados nos notebooks citados.

## III. Explica√ß√µes e Justificativas sobre o case desenvolvido.
Cada notebook cont√©m **explica√ß√µes e justificativas** sobre sua utiliza√ß√£o e implementa√ß√£o, proporcionando uma vis√£o clara do processo.

## IV. Melhorias e Considera√ß√µes Finais.

### Melhorias
Ao longo do desenvolvimento deste case, percebo oportunidades para aprimoramento, incluindo a explora√ß√£o de diferentes formatos de importa√ß√£o de bases de dados, al√©m da integra√ß√£o com APIs, o que permitiria um maior dinamismo na ingest√£o de dados.
Outro ponto relevante seria a visualiza√ß√£o dos dados em um dashboard no Power BI. Embora a cria√ß√£o de dashboards n√£o seja uma fun√ß√£o direta do engenheiro de dados, considero valioso compreender sua aplica√ß√£o para enriquecer as an√°lises e facilitar a interpreta√ß√£o dos resultados. 
Esses aprimoramentos contribuiriam para tornar o case ainda mais completo e alinhado √†s demandas do mercado.

### Considera√ß√µes Finais
Durante o desenvolvimento deste case, tive a oportunidade de explorar conceitos novos e aprofundar meu entendimento em diversas etapas e processos da engenharia de dados. Esse processo n√£o apenas ampliou minha vis√£o sobre o tema, mas tamb√©m me proporcionou novos aprendizados, fortalecendo meu conhecimento e aprimorando minhas habilidades. Foram meses de estudo intenso, nos quais pude perceber ainda mais a presen√ßa dos dados no dia a dia e sua relev√¢ncia em diferentes √°reas. Mesmo atuando como desenvolvedora low-code, o aprendizado adquirido sobre engenharia de dados e a √°rea de dados em geral tem impactado positivamente minhas entregas como desenvolvedora de software, aprimorando minha capacidade de an√°lise e tomada de decis√£o.

Este projeto representa apenas o in√≠cio da minha jornada no universo dos dados, e sei que ainda h√° muito a ser explorado e aprendido. A experi√™ncia adquirida aqui fortaleceu minha determina√ß√£o em continuar estudando e evoluindo, buscando sempre novas formas de aplicar esse conhecimento de maneira eficiente e inovadora.


